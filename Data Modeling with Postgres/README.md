# Sparkify Postgres ETL


## Project Goal

1	Discuss the purpose of this database in the context of the startup, Sparkify, and their analytical goals.
2	State and justify the database schema design and ETL pipeline.


## Context

A startup called Sparkify wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. 
The analytics team is particularly interested in understanding what songs users are listening to. 
Currently, they don't have an easy way to query their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

This project is meant to extract, transform and loads 5 main informations (tables) from the Sparkify app (an app to listen to your favorite musics) logs:
 - `users`
 - `songs`
 - `artists`
 - `songplays`
 - `time` 
 
## Datasets

**Song Dataset**
The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.
song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json
And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}

**Log Dataset**
The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.
The log files in the dataset we'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json

## Database Schema

Here we use a star schema with one main fact table containing all the measures associated to each event (user song plays), 
and 4 dimentional tables, each with a primary key that is being referenced from the fact table.

### Song Plays table

- *Name:* `songplays`
- *Type:* Fact table
- *Attributes*:
- songplay_id (INT) PRIMARY KEY: ID of each user song play 
- start_time TIMESTAMP: Timestamp of beggining of user activity
- user_id (INT) NOT NULL: ID of user
- level (VARCHAR): User level {free | paid}
- song_id (VARCHAR) NOT NULL: ID of Song played
- artist_id (VARCHAR) NOT NULL: ID of Artist of the song played
- session_id (INT): ID of the user Session 
- location (VARCHAR): User location 
- user_agent (VARCHAR): Agent used by user to access Sparkify platform

### Users table

- *Name:* `users`
- *Type:* Dimension table
- user_id (INT) PRIMARY KEY: ID of user
- first_name (VARCHAR) NOT NULL: Name of user
- last_name (VARCHAR) NOT NULL: Last Name of user
- gender (VARCHAR): Gender of user {M | F}
- level (VARCHAR): User level {free | paid}


### Songs table

- *Name:* `songs`
- *Type:* Dimension table
- *Attributes*:
- song_id (VARCHAR) PRIMARY KEY: ID of Song
- title (VARCHAR) NOT NULL: Title of Song
- artist_id (VARCHAR) NOT NULL: ID of song Artist
- year (INT): Year of song release
- duration (FLOAT) NOT NULL: Song duration in milliseconds



### Artists table

- *Name:* `artists`
- *Type:* Dimension table
- *Attributes*:
- artist_id (VARCHAR) PRIMARY KEY: ID of Artist
- name (VARCHAR) NOT NULL: Name of Artist
- location (VARCHAR): Name of Artist city
- lattitude (FLOAT): Lattitude location of artist
- longitude (FLOAT): Longitude location of artist


### Time table

- *Name:* `time`
- *Type:* Dimension table
- *Attributes*:
- start_time (TIMESTAMP) PRIMARY KEY: Timestamp of row
- hour (INT): Hour associated to start_time
- day (INT): Day associated to start_time
- week (INT): Week of year associated to start_time
- month (INT): Month associated to start_time 
- year (INT): Year associated to start_time
- weekday (VARCHAR): Name of week day associated to start_time



## Project structure

Files used on the project:
1. **data** folder nested at the home of the project, where all needed jsons reside.
2. **sql_queries.py** contains all your sql queries, and is imported into the files below.
3. **create_tables.py** drops and creates tables. You run this file to reset your tables before each time you run your ETL scripts.
4. **test.ipynb** displays the first few rows of each table to let you check your database.
5. **etl.ipynb** reads and processes a single file from song_data and log_data and loads the data into your tables. 
6. **etl.py** reads and processes files from song_data and log_data and loads them into your tables. 
7. **README.md** current file, provides discussion on my project.