{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The aim of the project to create an ETL pipeline script to map Immigration and Airport data to an star schema data base as a source of truth tables to enable Analysis of data in an optimized manner. \n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3b194e015ad0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mudf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStringType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTimestampType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "# Do all imports and installs here\n",
    "import configparser\n",
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType, TimestampType\n",
    "from datetime import datetime, date, time, timedelta\n",
    "import glob\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['AWS_ACCESS_KEY_ID']='AKIAIK2OP6CMX5ZWKVQA'\n",
    "# os.environ['AWS_SECRET_ACCESS_KEY']='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'\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['aws']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['aws']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "# using two packages, one for reading database storage file sas7bdat, and one for s3 connection\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11,org.apache.hadoop:hadoop-aws:2.7.0\" )\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scope of the Project\n",
    "We first clean up the Immigration and Airport data. Next, we extract valid us related records by joining them with US State table. Then, we use US Immigration, Us Airport and Us state tables to created dimension tables. Finally, We create fact tables by joining the first two tables. Lastly, We perform quality checks on the data.\n",
    "\n",
    "#### Data Description\n",
    "For this project, 3 datasets are used\n",
    "\n",
    "*I94 Immigration Data* : This data comes from the US National Tourism and Trade Office. and it includes international visitor arrival statistics on select countries, type of visa, mode of transportation, age groups, states visited (first intended address only). source website (https://travel.trade.gov/research/reports/i94/historical/2016.html)\n",
    "\n",
    "*US State Table* : This table has a list of valid us state names and their abbreviations. source website: (http://worldpopulationreview.com/states/state-abbreviations/ )\n",
    "\n",
    "*Airport Code Table* : This is a simple table of airport codes and corresponding cities. source website (https://datahub.io/core/airport-codes#data)\n",
    "\n",
    "The aim of the project to create source of truth tables in order to analyze US immigration and airport data using star schema model. We first clean up the Immigration and Airport data. Next, we extract valid us related records by joining them with US State table. Then, we use US Immigration, Us Airport and Us state tables to created dimension tables. Finally, We create fact tables by joining all three tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumption for the project: in this project only immigration file is used, in order to process all avaialble data, simply us immigration_files\n",
    "# Immigration Data\n",
    "immigration_files= glob.glob('../../data/18-83510-I94-Data-2016/*.sas7bdat')\n",
    "immigration_fname = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "\n",
    "immigration_df = spark.read.format('com.github.saurfang.sas.spark').load(immigration_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_fname = 'state_abbreviation.csv'\n",
    "df_state =spark.read.format('csv').option('header', 'True').load(state_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Airport Code Data\n",
    "airport_fname = 'airport-codes_csv.csv'\n",
    "airport_df =spark.read.format('csv').option('header', 'True').load(airport_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+----------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+------------+-----+--------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|   arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear|dtaddto|gender|insnum|airline|      admnum|fltno|visatype|\n",
      "+-----+------+------+------+------+-------+----------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+------------+-----+--------+\n",
      "|  7.0|2016.0|   4.0| 254.0| 276.0|    ATL|2016-04-07|    1.0|     AL|   null|  25.0|    3.0|  1.0|20130811|     SEO| null|      G|   null|      Y|   null| 1991.0|    D/S|     M|  null|   null|3.73679633E9|00296|      F1|\n",
      "+-----+------+------+------+------+-------+----------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+------------+-----+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cleaning immigration data\n",
    "# converting the spark dataframe column to a list\n",
    "valid_state_codes = list(df_state.select('code').toPandas()['code'])\n",
    "\n",
    "# udf function to map invalid column values to other\n",
    "@udf(StringType())\n",
    "def validate_state(i94addr):  \n",
    "    if i94addr in valid_state_codes:\n",
    "        return i94addr\n",
    "    return 'other'\n",
    "\n",
    "\n",
    "\n",
    "@udf(StringType())\n",
    "def parse_date(arrdate):\n",
    "    if arrdate:\n",
    "        return (datetime(1960, 1, 1).date() + timedelta(arrdate)).isoformat()\n",
    "    return None\n",
    "\n",
    "# droping any missing values\n",
    "immigration_df_valid = immigration_df.dropna(how='any', subset=['i94port', 'i94addr'])\n",
    "\n",
    "# extracting valid state \n",
    "immigration_df_valid = immigration_df_valid.withColumn(\"i94addr\", validate_state(immigration_df_valid.i94addr))\n",
    "# extract arrival_date in standard format\n",
    "immigration_df_valid = immigration_df_valid.withColumn(\"arrdate\", parse_date(immigration_df_valid.arrdate))\n",
    "\n",
    "# only keep us related immigration data\n",
    "immigration_df_valid = immigration_df_valid.filter(immigration_df_valid.i94addr !='other')\n",
    "immigration_df_valid.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+-----+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|state|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+-----+\n",
      "|  00A|     heliport|   Total Rf Heliport|          11|       NA|         US|     US-PA|    Bensalem|     00A|     null|       00A|-74.9336013793945...|   PA|\n",
      "| 00AA|small_airport|Aero B Ranch Airport|        3435|       NA|         US|     US-KS|       Leoti|    00AA|     null|      00AA|-101.473911, 38.7...|   KS|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cleaning airport data\n",
    "airport_df_valid = airport_df.dropna(how=\"any\", subset=['iso_region', 'iso_country', 'local_code'])\n",
    "\n",
    "# keep US countries only\n",
    "airport_df_valid = airport_df_valid.filter(airport_df_valid.iso_country==\"US\")\n",
    "\n",
    "# extract state abbreviation, e.g. US-FL > FL \n",
    "get_state_name = udf(lambda x: x.split('US-')[1])\n",
    "airport_df_valid = airport_df_valid.withColumn('state', get_state_name(airport_df_valid.iso_region))\n",
    "# use udf function to filter valid states\n",
    "airport_df_valid = airport_df_valid.withColumn(\"state\", validate_state(airport_df_valid.state))\n",
    "\n",
    "# only keep us related airports\n",
    "us_airport_df = airport_df_valid.filter(airport_df_valid.state !='other')\n",
    "us_airport_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model && Data dictionary\n",
    "Star schema is chosen as the data model because it is simple and yet effective. users can write simple queries by joing fact and dimension tables to analyze the data.\n",
    "\n",
    "Following are the tables of the schema, along with their data dictionaries\n",
    "* Immigration Dimension table \n",
    "    - cicid:  identifier \n",
    "    - i94yr:  year\n",
    "    - i94mon: numeric month\n",
    "    - i94cit: origin city code\n",
    "    - i94res: residential code\n",
    "    - i94port: port code of destination city\n",
    "    - arrdate: arrival date , Y-m-d format\n",
    "    - i94mode: travel mode code, i.e. land, flight, etc\n",
    "    - i94addr: final state destination in us, filtered to only valid states\n",
    "    - depdate: Departure Date from the USA\n",
    "    - i94bir:  age\n",
    "    - i94visa: visa code\n",
    "    - count:   stat\n",
    "    - dtadfile: Character Date Field - Date added to I-94 Files\n",
    "    - visapost: department that issued visa\n",
    "    - occup: occupation\n",
    "    - entdepa: Arrival Flag\n",
    "    - entdepd: Departure Flag\n",
    "    - entdepu: Update Flag\n",
    "    - matflag: Match flag\n",
    "    - biryear: birth year\n",
    "    - dtaddto: Date to which admitted to U.S.\n",
    "    - gender:  gender\n",
    "    - insnum: insurance number\n",
    "    - airline: airline used\n",
    "    - admnum: admission number\n",
    "    - fltno: flight number\n",
    "    - visatype: Class of admission legally admitting the non-immigrant to temporarily stay in U.S\n",
    "    \n",
    "    \n",
    "* Airport dimension table\n",
    "    - ident: table identifier\n",
    "    - type: type of airport\n",
    "    - name: airport name\n",
    "    - elevation_ft: elevation in feet\n",
    "    - continent: continent\n",
    "    - iso_country: country\n",
    "    - municipality: city\n",
    "    - gps_code: gps code\n",
    "    - iata_code: not sure\n",
    "    - local_code: port code\n",
    "    - coordinates: longitute, latitude\n",
    "    - state: state its located in\n",
    "    \n",
    "    \n",
    "* State dimension table\n",
    "    - state: state full name\n",
    "    - abbrev: state abbreviation\n",
    "    - code: state code\n",
    "    \n",
    "    \n",
    "* Fact table\n",
    "    - year\n",
    "    - month\n",
    "    - arrival_date\n",
    "    - settled_state\n",
    "    - airport_code\n",
    "    - gender\n",
    "    - airport_state\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "Here are the steps necessary to pipeline the data into the star schema:\n",
    "\n",
    "* Immigration Dimension table is created from cleansed dataframes and then saved to a parquet file on s3, partitioned by year,month, and state. (\"i94yr\", \"i94mon\", \"i94addr\")\n",
    "* Airport Dimension table is created from cleansed dataframes and then saved to a parquet file on s3, partitioned them by state.\n",
    "* State Dimension table is created from dataframes and then saved to a parquet file on s3.\n",
    "* Fact table is created from joining the immigration and airport table on state and port and then it is saved to a parquet file on s3, partitioned by year, month, and airport_state.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"s3a://dend-capstone/data-model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create view & fact table\n",
    "immigration_df_valid.createOrReplaceTempView(\"immigration\")\n",
    "us_airport_df.createOrReplaceTempView(\"airport\")\n",
    "df_state.createOrReplaceTempView(\"state\")\n",
    "\n",
    "fact_table = spark.sql(\"\"\"\n",
    "    SELECT i94yr as year, \n",
    "           i94mon as month,\n",
    "           arrdate as arrival_date,\n",
    "           i94addr as immigration_state,\n",
    "           i94port as airport_code,\n",
    "           gender,\n",
    "           state as airport_state\n",
    "    FROM immigration i\n",
    "    JOIN airport a\n",
    "    ON i.i94port == a.local_code\n",
    "\"\"\").dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write dimmension tables to s3\n",
    "immigration_df_valid.\\\n",
    "    write.mode(\"overwrite\").\\\n",
    "    parquet(os.path.join(output_path , 'immigration.parquet')).\\\n",
    "    partitionBy(\"i94yr\", \"i94mon\", \"i94addr\")\n",
    "\n",
    "us_airport_df.\\\n",
    "    write.mode(\"overwrite\").\\\n",
    "    parquet(os.path.join(output_path , 'airport.parquet')). \\\n",
    "    partitionBy(\"state\")\n",
    "    \n",
    "df_state.\\\n",
    "    write.mode(\"overwrite\").\\\n",
    "    parquet(os.path.join(output_path , 'state.parquet'))\n",
    "\n",
    "# write fact table to s3\n",
    "fact_table.\\\n",
    "    write.mode(\"overwrite\").\\\n",
    "    parquet(os.path.join(output_path , 'fact.parquet')).\\\n",
    "    .partitionBy(\"year\", \"month\", \"airport_state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    " \n",
    "for quality check, we can verify the data in the fact dataframe. \n",
    "we can also verify our writing process to s3, by reading parquet file and performing a count. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------------+-----------------+------------+------+-------------+\n",
      "|  year|month|arrival_date|immigration_state|airport_code|gender|airport_state|\n",
      "+------+-----+------------+-----------------+------------+------+-------------+\n",
      "|2016.0|  4.0|  2016-04-01|               TX|         DAL|     M|           TX|\n",
      "|2016.0|  4.0|  2016-04-03|               TX|         HOU|     F|           TX|\n",
      "|2016.0|  4.0|  2016-04-06|               NY|         HPN|     M|           NY|\n",
      "|2016.0|  4.0|  2016-04-11|               UT|         SLC|     M|           UT|\n",
      "|2016.0|  4.0|  2016-04-11|               FL|         OPF|     F|           FL|\n",
      "+------+-----+------------+-----------------+------------+------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_table.createOrReplaceTempView(\"fact\")\n",
    "arrived_settled_same_state = spark.sql(\"\"\"\n",
    "    select * \n",
    "    from fact\n",
    "    where fact.immigration_state == fact.airport_state\n",
    "\"\"\")\n",
    "arrived_settled_same_state.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in state table:  51\n",
      "Number of records in airport table:  21236\n"
     ]
    }
   ],
   "source": [
    "# verify the data in our parquet files\n",
    "state_parquet = spark.read.parquet(output_path + \"state.parquet\")\n",
    "airport_parquet = spark.read.parquet(output_path + \"airport.parquet\")\n",
    "\n",
    "print (\"Number of records in state table: \", state_parquet.count())\n",
    "print (\"Number of records in airport table: \", airport_parquet.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* **Clearly state the rationale for the choice of tools and technologies for the project.**\n",
    "    -  I used Apache Spark to read, clean, transform, and create parquet files. Spark's schema-on-read is a powerful tool that let me do all the transformation without using any additional database. Using spark, I could process the raw data as if I am working on a traditional dtabase\n",
    "    \n",
    "    \n",
    "* **Propose how often the data should be updated and why.**\n",
    "    - since the organization stores data every month, this etl pipeline can be updated monthly.\n",
    "    \n",
    "    \n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * **The data was increased by 100x.**\n",
    "     - We can use a resource manager that schedule these jobs accross clusters. we can run spark's yarn mode and and increase the number of nodes available on the cluster\n",
    " * **The data populates a dashboard that must be updated on a daily basis by 7am every day.**\n",
    "     - ETL pipeline related to immigration should be scheduled using an orchestration tool like Apache Airflow overnite. Other transformations related to airport and state can be scheduled for yearly update due to minimal update\n",
    " \n",
    " * **The database needed to be accessed by 100+ people.**\n",
    "     - We can use a resource manager that schedule these jobs accross clusters. we can run spark's yarn mode and increase the number of nodes available on the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
